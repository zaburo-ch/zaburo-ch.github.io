<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>つくった on ZABURO app</title>
    <link>https://zaburo-ch.github.io/categories/%E3%81%A4%E3%81%8F%E3%81%A3%E3%81%9F/</link>
    <description>Recent content in つくった on ZABURO app</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Written by ZABURO</copyright>
    <lastBuildDate>Thu, 14 May 2015 22:36:00 +0900</lastBuildDate>
    
	<atom:link href="https://zaburo-ch.github.io/categories/%E3%81%A4%E3%81%8F%E3%81%A3%E3%81%9F/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Wikipediaの記事でPageRankを計算してみた。</title>
      <link>https://zaburo-ch.github.io/post/20150514_1/</link>
      <pubDate>Thu, 14 May 2015 22:36:00 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/20150514_1/</guid>
      <description>PageRankの記事の続きです。
【Python】PageRankアルゴリズム
PageRank実装だけして終わるのももったいないので
Wikipediaのページ内でPageRank計算してみました。
コードはこちら
zaburo-ch/wikipedia_analysis
https://github.com/zaburo-ch/wikipedia_analysis
前に作ったwikipedia.pyのコードとまとめてレポジトリにしました。
pagerank_wiki.pyを実行すると次のような動作をします。
・http://dumps.wikimedia.orgより1日分のデータを取得
・国コードがjaのもののうち閲覧数上位10000ページを取り出す
・各ページの記事内のリンクをエッジとした有向グラフつくる
・有向グラフ内でPageRankを計算し大きい順に出力する
やってることはかなりwikipedia.pyに近いですが、
見返してみるとあまりにもなコードだったのでかなり書き換えました。
他にもリンク抽出にHTMLParser使ってみたりいろいろ変えてます。
前のやつと違って計算の部分にそれほど時間がかからないので
スペック次第ですが少なくとも寝てる間くらいには終わると思います。
リンク解析もこっちのほうがかなり早いです。
さて、肝心の実行結果はこんな感じです。

wikipedia_analysis/pagerank_wiki_data/result.txt
「特別:カテゴリ」がぶっちぎりで大きいですね。
確認してみたところほぼ全てのページの一番下に
「特別:カテゴリ」へのリンクがあったのでたぶんそのせいです。
「Wikipedia:出典を明記する」とかもこの類いかな。
「日本」や年がおおいのは人とか何かの作品の記事の
テンプレート化している右の枠内で登場することが多いためだと思います。
テレビ局が上位に多いのはテレビ番組からほぼ確実にリンクされているのと
閲覧数上位10000ページで限定しているため
そのなかでテレビ番組が多かったためではないかと考えています。
上位はほとんどが誰でも知っているような一般的な言葉なんですが、
101位のインターネット・ムービー・データベースってやつ、
全然何のことかわからないです。
「集英社」とか「台湾」より上なので、
かなり多くページがここにリンクしてるはずなんですが、
これどんなページからリンクされてるんでしょう？
全体的にPageRankはWikipediaのページの重要性判断としては
あまり向いていないように思いました。
WikipediaにはPageRankの基本的な考え方が合ってないですしね。
ほかのWebサイトでも試してみればもっと面白い結果が得られるかもしれません。
機会があればやってみたいと思います。</description>
    </item>
    
    <item>
      <title>【Python】PageRankアルゴリズム</title>
      <link>https://zaburo-ch.github.io/post/20150514_0/</link>
      <pubDate>Thu, 14 May 2015 20:54:05 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/20150514_0/</guid>
      <description>PageRankというアルゴリズム、
以前からなんとなくは知ってはいたのですが、
ランダムサーファーモデルで計算する方法を聞いて、
めちゃくちゃ賢いなこれーって思ったので実際にやってみました。
ひとまずPageRankについて調べたことを纏めます。
PageRankは基本的に次の２つの考え方でページの重要度を推定します。
・多くのページからリンクされているページの質は高い
・質の高いページからリンクされているページの質は高い
これを数学的に考えるのにランダムサーファーモデルを利用します。
ランダムサーファーモデルでは
ページのリンク関係を有向グラフとして考え、
人(サーファー)にこの有向グラフをランダムに辿らせたとき、
人が居る確率が高いページを重要とします。
まず、ページの総数をNとし、N次正方行列Mを
M[i][j] = ページjにいるサーファーがページiに移動する確率
と定義します。
例えばページ0にページ1とページ5へのリンクしかないとすると、
サーファーはランダムに移動するので
M[i][0]は i=1or5 のとき1/2 となりそれ以外のiでは0となります。
また、ベクトルP(t)を時刻tに各ページに人がいる確率とすると
P(0)は全ての要素が 1/N のベクトルとなり
P(t+1)はMとP(t)の積を取ることで計算できます。
有向グラフが強連結のとき、この遷移を無限に繰り返すことで
Pはtに依らない一定の値に収束します。よってこのPは
M P = P
の解を要素の和が1になるよう正規化してあげることにより求められます。
このPの大きさが各ページの重要度となります。
行列計算において Ax = λx を解く、というのは固有値問題と言って
色々な方法が考えられているらしいのですが、ここについては
行列計算における高速アルゴリズム
http://www.cms-initiative.jp/ja/events/0627yamamoto.pdf
こちらのページが詳しいです。
Pythonではscipy.sparse.linalg.eigsを使うと
Implicitly Restarted Arnoldi で計算してくれます。
ただ、M P = P を解くだけのためにこれらを使うのが速いのかは
勉強不足で僕もよくわかっていません。
実際のウェブページでは、ページ間の関係を有効グラフにしても
必ずしも上で述べたような強連結になるわけではありません。
そのため「一定の確率でサーファーはリンクを辿らずにランダムに移動する」
という考えを新たに導入します。その時はMにあたるものを
全ての要素が 1/N である N次正方行列Uと普通にリンクを辿る確率αを用いて
(αM + (1-α)U) として計算することによってPが求められます。
次の図のようなリンク関係にあるページのPageRankを</description>
    </item>
    
    <item>
      <title>Mac OS Xで時々ジェスチャーが反応しなくなる</title>
      <link>https://zaburo-ch.github.io/post/20150324_0/</link>
      <pubDate>Tue, 24 Mar 2015 17:21:45 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/20150324_0/</guid>
      <description>普段ウインドウの切り替えはMission Controlでやっているのですが、
ときどきジェスチャーが反応しなくなってしまって、
仮想デスクトップの切り替えもできなくなるものですから
再起動しなければならなくなる、というのが近頃起こります。
原因はよくわからないのですが、
OSまるごと再起動しなくてもDockを再起動させてやるだけで
なんとかなるみたいです。
ps -x | grep Dockで出てきたPIDを指定してkillしてやると
自動で起動してくれるようなのでこれで済ませていますが
面倒なので実行ファイルへの変換の勉強がてら
ちょっと書いてアプリにしてみました。

こんな感じでDockに配置して使っています。
アイコン画像は以下のものを利用させて頂きました。
http://www.easyicon.net/language.ja/1088483-stop_icon.html
ソース等はこちら
https://github.com/zaburo-ch/DockKiller
こちらからダウンロードしてご利用ください↓
DockKiller-1.0.dmgをダウンロード</description>
    </item>
    
    <item>
      <title>Wikipediaのページ解析に使ったpythonコード</title>
      <link>https://zaburo-ch.github.io/post/20141130_0/</link>
      <pubDate>Sun, 30 Nov 2014 01:36:54 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/20141130_0/</guid>
      <description>すっかりわすれていましたがソースコードです。
とりあえずpython触ってみようくらいの気持ちで書いたコードなので
pythonに慣習みたいなものがあるならたぶんそれには従えていません。
multiprocessing、numpy、pandasあたりをちゃんと使えば
格段に早くすることもできるかもしれません。やんないけど。
python wikipedia.py 20141101
のようにして日付指定して使います。
以下のようなことをやってます。
・http://dumps.wikimedia.orgから1時間ごとの閲覧数のデータを1日分取ってくる
・国コード(?)がjaの物だけ抽出する
・標準ライブラリのCounterで各ページの1日分の閲覧数をカウントする
・閲覧数上位10000ページを取り出す
・1ページずつ開き記事内の/wiki/で始まるリンクを抽出する
・リンクがあれば距離1なければINFとして(ディクショナリで)隣接行列をつくる
・ワーシャルフロイド法で全点間最短距離を求める
・ソートして表示

Gistを使ってみました。綺麗に表示してくれますね。
過去の物をGistに置き換えたりはしませんが
今後はできるだけこれをつかっていこうと思います。</description>
    </item>
    
    <item>
      <title>Wikipediaのアクセス上位10000ページの距離を計算した話</title>
      <link>https://zaburo-ch.github.io/post/20141017_0/</link>
      <pubDate>Fri, 17 Oct 2014 23:44:17 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/20141017_0/</guid>
      <description>先日の記事の続きです。
Wikipediaでリンクを辿って遊ぶ
今回探したのはWikipediaに存在する記事のうち
最も距離のはなれている記事の組み合わせです。
ここで言う距離とは、ある記事から記事内のリンクのみを辿って行く時
何回リンクを踏めばその記事に辿り着くかを表した数字です。
複数あるルートのうち最も短いものをその組の距離としました。
さて、Wikipediaについてですが、
Wikipedia:全言語版の統計
http://ja.wikipedia.org/wiki/Wikipedia:%E5%85%A8%E8%A8%80%E8%AA%9E%E7%89%88%E3%81%AE%E7%B5%B1%E8%A8%88
によれば、2014年10月17日現在
純記事数は全言語総計でなんと33,448,472もあるそうです。
日本語のものだけで930,619もあるので
これをすべて解析するとなると人生が終わるまでに
計算し終わるかという問題が出てきます。
というわけで今回は、日本語版Wikipediaの記事のうち
ある日の閲覧数が上位の記事を一定数とって来て
その記事のみを対象にページ間距離を計算しました。
技術的なことは後日記事にするとして
早速結果を書いていきたいと思います。
まず2014年9月1日の閲覧数上位1000位に限定した場合の結果。
上位20位のみを抜粋しました。
&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;#45;&amp;ndash;
List of longest distance between Wikipedia pages
20140901 : Page view Top 1000
1: 8 続柄 -&amp;gt; 近江友里恵
2: 8 続柄 -&amp;gt; たちかぜ自衛官いじめ自殺事件
3: 8 国鉄105系電車 -&amp;gt; たちかぜ自衛官いじめ自殺事件
4: 8 七草 -&amp;gt; 近江友里恵
5: 8 七草 -&amp;gt; たちかぜ自衛官いじめ自殺事件
6: 8 カルトの集団自殺 -&amp;gt; 近江友里恵
7: 8 カルトの集団自殺 -&amp;gt; たちかぜ自衛官いじめ自殺事件</description>
    </item>
    
    <item>
      <title>とんぺーのミスコンにエントリーした気分になれるブックマークレットつくった</title>
      <link>https://zaburo-ch.github.io/post/20141007_0/</link>
      <pubDate>Tue, 07 Oct 2014 22:40:04 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/20141007_0/</guid>
      <description>使い方
1.幅140px高さ260pxの画像を用意する。
2.画像のURLをこのテキストボックスに入れる。

3.このリンクを右クリックしてブックマーク
学祭ミスコン ブックマークレット
4.コンテストの公式ページに行く
第6回Mr.&amp;amp;Ms.東北大コンテスト
http://www.festa-tohoku.org/program/contest/contest.html
5.先ほどのブックマークを開く！
5人目のミス候補が現れます。
マウスを画像にのせたときの白いオーバーレイは実装しましたが、
名前やNo.5の記述は省略しました。
以下に元のコードを載せるので、誰かやって！
 var imgurl = &amp;lsquo;ms1.jpg&amp;rsquo;; var msul = $(&amp;lsquo;ul&amp;rsquo;).eq(1); msul.children().attr(&amp;lsquo;style&amp;rsquo;,&amp;lsquo;margin-right:25px;&amp;lsquo;); var newms = $(&amp;rsquo;&amp;lt;li&amp;gt;&amp;rsquo;); newms.css({ &amp;lsquo;position&amp;rsquo;:&amp;lsquo;relative&amp;rsquo;, &amp;lsquo;float&amp;rsquo;:&amp;lsquo;left&amp;rsquo;, &amp;lsquo;width&amp;rsquo;:&amp;lsquo;150px&amp;rsquo;, &amp;lsquo;height&amp;rsquo;:&amp;lsquo;270px&amp;rsquo;, &amp;lsquo;background-image&amp;rsquo;:&amp;lsquo;-moz-linear-gradient(top,#931f20,#f29d80 50%,#931f20)&amp;rsquo; }); newms.css(&amp;lsquo;background-image&amp;rsquo;,&amp;lsquo;-webkit-gradient(linear,left top,left bottom,from(#931f20),color-stop(0.5, #f29d80),to(#931f20))&amp;rsquo;); var msimg = $(&amp;rsquo;&amp;lt;img&amp;gt;&amp;rsquo;); msimg.attr(&amp;lsquo;src&amp;rsquo;,imgurl); msimg.css({ &amp;lsquo;position&amp;rsquo;:&amp;lsquo;absolute&amp;rsquo;, &amp;lsquo;top&amp;rsquo;:&amp;lsquo;5px&amp;rsquo;, &amp;lsquo;left&amp;rsquo;:&amp;lsquo;5px&amp;rsquo;, }); newms.append(msimg); var overlay = $(&amp;rsquo;&amp;lt;div&amp;gt;&amp;rsquo;); overlay.css({ &amp;lsquo;position&amp;rsquo;:&amp;lsquo;absolute&amp;rsquo;, &amp;lsquo;top&amp;rsquo;:&amp;lsquo;4px&amp;rsquo;, &amp;lsquo;left&amp;rsquo;:&amp;lsquo;4px&amp;rsquo;, &amp;lsquo;width&amp;rsquo;:&amp;lsquo;142px&amp;rsquo;, &amp;lsquo;z-index&amp;rsquo;:&amp;lsquo;100&amp;rsquo;, &amp;lsquo;height&amp;rsquo;:&amp;lsquo;262px&amp;rsquo;, &amp;lsquo;background&amp;rsquo;:&amp;lsquo;rgba(255,255,255,0.15)&amp;rsquo; }); overlay.hide(); newms.append(overlay); newms.hover( function(){ overlay.show(); }, function(){ overlay.</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>機械学習 on ZABURO app</title>
    <link>https://zaburo-ch.github.io/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/</link>
    <description>Recent content in 機械学習 on ZABURO app</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <copyright>Written by ZABURO</copyright>
    <lastBuildDate>Wed, 29 Jun 2016 13:06:24 +0900</lastBuildDate>
    
	<atom:link href="https://zaburo-ch.github.io/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>t-SNE の逆変換を試してみた</title>
      <link>https://zaburo-ch.github.io/post/tsne-decode/</link>
      <pubDate>Wed, 29 Jun 2016 13:06:24 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/tsne-decode/</guid>
      <description>Parametric t-SNEでt-SNEの変換をニューラルネットで近似することができたので、
その逆についてもやってみました。
逆変換と言っても特に難しいことはやっておらず、
まず普通にBarnes-Hut t-SNEで訓練データを2次元に変換して、
変換後の座標を入力、変換前の座標を教師データとして教師あり学習を行います。
今回は、変換後の座標のうち訓練データにないような座標について、
どのように逆変換されるのかが気になるので汎化性能を高めるためにDropoutを入れました。
コードは次の通りです。

t-SNEの結果がこんな感じで、
適当に座標を指定して逆変換した結果がこれ
かなり綺麗に逆変換できました。
deconvolutionを使えばもっとうまく逆変換できるかもしれませんが、
MNISTの結果としてはこれで十分だと思います。
なんとかしてこの方針で画像生成とかできないのかなーと思っているのですが、
まずはグレースケールでない画像をうまく変換できるようにt-SNE側を工夫しなければいけない気がします。</description>
    </item>
    
    <item>
      <title>Parametric t-SNE を Keras で書いた</title>
      <link>https://zaburo-ch.github.io/post/parametric-tsne-keras/</link>
      <pubDate>Tue, 28 Jun 2016 22:01:32 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/parametric-tsne-keras/</guid>
      <description>最近、t-SNEについていろいろ調べいて、その中でParametric t-SNEの論文を読みました。
元のt-SNEは可視化や次元削減の手法としてとても有用なのですが、 変換後の座標を乱数で初期化し、
KLダイバージェンスが小さくなるように勾配降下で座標を調整していく感じなので、
初めの乱数次第で配置は大きく変わりますし、別なデータを同じような場所に投射するようなことができません。
そのため、kaggleなどで前処理として使われる際には、
訓練データとテストデータをくっつけて変換するという方法が取られています。
しかし、本来見れないはずのテストデータを訓練データを変換する時にも使うというのはグレーな感じがします。
そこで、座標を直接調整するのではなく、
元の座標をパラメトリックな関数で低次元の座標に投射するようにして、
その関数のパラメータを学習してあげようというのがParametric t-SNEです。
ここで、関数としてニューラルネットが使われます。
論文では、RBMを重ねてpre trainingしてfine tuningというのをやっているのですが、
どうせやるならということで今風にReLUで書きました。
コードはここに置いてあります。
とりあえず論文にも載っているMNISTで試しました。
100 epoch回すとAWS EC2のg2.2xlargeインスタンスでだいたい10分程度かかります。
普通のMNISTなので60000件の訓練データと10000件のテストデータがあります。
学習していく過程が見れたら面白そうだなと思ったので、
各epoch終了後テストデータに対して変換を行い、散布図を書くようにしました。
結果はこんな感じ。
(なんかgifが吐けなくてmp4をgifに変換したので画質が悪い&amp;hellip;)
訓練に使っていないデータに対してすごくいい感じに別けられていると思います。
10~20 epochくらいでいい感じに別けられているので、
10 epoch毎とかにミニバッチをシャッフルしてあげるともっと良くなるかもしれません。
一応shuffle_intervalという変数が用意してあって、
shuffle_interval回のepochが回るとミニバッチがシャッフルされてPが再計算されます。
Pを計算する部分についてもPython上でですが並列化してあるので少しは早いと思います。
Convolutional Parametric t-SNEだー！！って言って畳み込み層を使ったものも書いたのですが、
普通のMLP版とあまり変わらなかったのでお蔵入りしました。
いつもMNISTしてばかりなのでCIFAR-10でも試してみたのですがあまりうまくいきませんでした。
そもそもt-SNEでCIFAR-10がうまくいくのが試していないので良くわかりませんが、
これConvolutionalしてなんとか解決できないかなーと考えています。
追記：
逆変換も試しました。</description>
    </item>
    
    <item>
      <title>FaxOCR を CNN でやってみた</title>
      <link>https://zaburo-ch.github.io/post/faxocr/</link>
      <pubDate>Wed, 13 Apr 2016 11:32:33 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/faxocr/</guid>
      <description>せっかくKerasを使ってみたのでCNNもやってみたいということで、
FaxOCRというMNISTと同形式のデータセットで手書き文字認識をやってみました。
ソースコードはここにあります。
このデータセットではtrain setがかなり小さいので、
まず最初に適当に拡大縮小や回転をして画像データの枚数を11倍に(1枚から10枚生成)しました。
CNNのアーキテクチャはtoshi-kさんのコードを参考にして適当に設定しました。
ただ、データセットが小さくて過学習してしまいそうだなーと思ったので、
上記のものに比べて小さいネットワークになっています。
入力したテンソルがどの時点でどのサイズになっているか確認する方法がわからなかったので、
ZeroPaddingなどがこれでうまくいっているのかわかりませんが、
入力(1, 28, 28) -&amp;gt; (64, 12, 12) -&amp;gt; (64, 12, 12) -&amp;gt; (256, 10, 10) -&amp;gt; (128) -&amp;gt; (10)出力
という形になっていると嬉しいなくらいの感じで書いています。
こういう風にデバッグする方法が知りたい&amp;hellip;&amp;hellip;
結果は次の通りです。
   dataset 正答率     train 99.5%   valid 99.1%   test 93.1%    増やしたデータセットのうち90%をtrain、10%をvalidとし、
trainで訓練してvalidが最も高くなるところで止まるようになっています。
もとのデータセット全部をtrainにしてtestが高くなるところで止めるともう少し精度が上がります。(これ)
GPUマシンを持っていないので学習に時間がかかりすぎりてあんまり色々試せない&amp;hellip;&amp;hellip;
[追記]
データセットのバージョンはfaxocr-numbers-20160411c.zipのものです。</description>
    </item>
    
    <item>
      <title>Pythonで Neural Fitted Q Iteration を実装する</title>
      <link>https://zaburo-ch.github.io/post/neural-fitted-q-iteration/</link>
      <pubDate>Thu, 17 Mar 2016 13:37:58 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/neural-fitted-q-iteration/</guid>
      <description>前々回に実装した多層パーセプトロンと前回実装した倒立振子のシミュレータを用いて、
Neural Fitted Q Iteration(NFQ)の実験を行います。
NFQはQ学習の最適行動価値関数を多層パーセプトロンを用いて近似する手法の一つで、
学習中にはデータを追加せず、事前に集められたデータのみから学習を行います。
コードはこんな感じ。MLPはkerasで構築しました。

mはepisodeの個数にあたる変数になっていて、
[50, 100, 150, 200, 300, 400]の各m対して、50回実験を行うようになっています。
実験の大まかな流れは、
 make_episodes(m)でm個のepisodeを作る
 多層パーセプトロンを構築
 Neural Fitted Q Iterationを実行
 倒立振子を立たせるタスクを実行
 停止するまでの時間を記録(t&amp;lt;299なら失敗、t==299なら成功)
  という感じです。肝心の3.では、
episodesの中からpattern_set_size個ずつepisodesを取り出し、
その中の各cycleから入力xと教師信号tを作成して、
多層パーセプトロンをこれにfitさせるのを繰り返すことで学習を行っています。
episodes1周だけではうまくタスクを成功させることができなかったので、
毎回取り出す順番をランダムに変えてepisodesを5周させるようにしています。
実験結果は次のようになりました。
   m 成功した回数     50 25 / 50 (50%)   100 41 / 50 (82%)   150 43 / 50 (86%)   200 43 / 50 (86%)   300 48 / 50 (96%)   400 46 / 50 (92%)    m = 50 の場合については論文とほぼ同程度成功できています。</description>
    </item>
    
    <item>
      <title>Pythonで 多層パーセプトロン を実装する</title>
      <link>https://zaburo-ch.github.io/post/mlp/</link>
      <pubDate>Sat, 20 Feb 2016 01:29:30 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/mlp/</guid>
      <description>前回は古典的Q学習を実装しましたが、次はニューラルネットを用いたQ学習として、
Neural Fitted Q Iterationを使ったQ学習を実装しようと考えています。
今回はその前の勉強として、
誤差逆伝播法を用いた多層パーセプトロンをNumPyだけで実装してみます。
誤差逆伝播法についてはこちらのスライドが詳しいです。

わかりやすいパターン認識_3章
コードはこんな感じ。 
活性化関数は全てシグモイド関数で、コスト関数は残差の平方和を用いました。
各層はforwardで出力を計算して、
backwardで前の層の誤差を計算しつつ W や b の更新を行います。
tanhなどの層も同じような関数を実装してMLPのinitを適宜変えれば使える(はず)。
DeepLearningTutorialsのMLPのコードっぽくしたくて、
DeepLearningTutorialsのDBNをNumPyで実装した方のコードとか、
NumPyでMLPを実装を実装した方のコードなどを参考に書きました。
各所に載っている式を見る限りは出力層も活性化関数を使うっぽいんですが、
Chainerの多層パーセプトロンのサンプルをはじめとして、
出力層で線形変換するだけ(活性化関数を使わない)のものが結構あって混乱しました。
たぶんシグモイド関数やtanhだと出力できる範囲が狭くて不便なので
出力層だけ恒等関数使ってるんだろうという感じでとりあえず考えています。
先のNFQの論文で、全ての層でシグモイドを使ったみたいなことが書いてあったので、
今回は出力層にもシグモイド関数を使うことにしています。
あと出力層での誤差も、出力層の活性化関数の微分をかけるのかどうかがわからなくて
結構悩みましたが、スライドの式に従ってかけることにしました。
また、確率的勾配降下法(SGD)でミニバッチを使った学習ができるように書きましたが、
ミニバッチを使う時に W や b の更新をどうやるのかがよくわからなくて、
結局ループ回して学習パターン1つずつ使って更新を行うようにしました。
出力層の時点で誤差の和をとってそれを伝播するんだと思ったのですが、
そしたらそれと掛け合わせる前の層の出力はどうするんだ！？ってなって
結局わからず、まあ結局やってること大体一緒でしょってことでこの形にしました。
結果はこんな感じになります。赤が教師信号、青がMLPの出力で、
左が1000回反復した場合、右が10000回反復した場合です。
たぶん学習できていると思うのですが、
いまいちうまくいっているのか確証が持てなかったので、
同じネットワークをChainerでも実装してみました。
コードはここ。Tutorialのものをちょっといじっただけです。
結果は次の通り。
Chainerの方が若干うまく近似できていますが、
概ね同じような感じの結果が得られたので、自分で書いた方も大丈夫なはず。
実行時間はNumPyだけの方がかなり早いです。
ただ自分で微分する必要もなく適当に層つなぐだけで出来ちゃったのでChainerすごい
2016/03/15 追記
Kerasでも試してみました。コードはここ。
結果はだいたい同じような感じ
後ろでTheanoが使われていることもありかなり速いです。
まだMLP書いただけなので、もっと大規模なモデルを実装するときにどうなるかはわかりませんが、
モデルの記述の仕方や、sklearnっぽいfit・predictの書き方など、
結構書きやすいように感じました。もっと使ってみたい。</description>
    </item>
    
    <item>
      <title>Pythonで Q学習 を実装する</title>
      <link>https://zaburo-ch.github.io/post/q-learning/</link>
      <pubDate>Sun, 14 Feb 2016 14:28:18 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/q-learning/</guid>
      <description>Deep Q-Networkについて調べてみたら面白い記事を見つけました。
DQNの生い立ち　＋　Deep Q-NetworkをChainerで書いた
http://qiita.com/Ugo-Nama/items/08c6a5f6a571335972d5
この記事を読んで、Deep Q-Networkが
Q学習 -&amp;gt; Q-Network -&amp;gt; Deep Q-Network という流れ生まれたものだということがわかりました。
この流れをPythonで実装しながら辿ってみようと思います。
今回はQ学習を実装します。
Q学習について下記のページに詳しく載っているので割愛します。
強化学習
http://www.sist.ac.jp/~kanakubo/research/reinforcement_learning.html
強化学習とは？
http://sysplan.nams.kyushu-u.ac.jp/gen/edu/RL_intro.html
まず、Q学習で適応する環境として次のような簡単な環境を考えます。
環境の状態は、&#39;A&#39;または&#39;B&#39;からなる長さ8の文字列で表され、 その文字列にはある法則により得点がつけられる。 プレイヤーはその法則についての知識を予め持たないが、 文字列中の任意の1文字を選んで&#39;A&#39;または&#39;B&#39;に置き換えることができ、 その結果、その操作による得点の変化量を報酬として受け取る。  たぶんマルコフ決定過程になっていると思います。マルコフ性、エルゴート性も持つはず。
文字列に得点をつける法則はなんでも良いのですが、
今回は、特定の文字列(単語)に次のように得点を割り当てて、
{&amp;ldquo;A&amp;rdquo;: 1, &amp;ldquo;BB&amp;rdquo;: 1, &amp;ldquo;AB&amp;rdquo;: 2, &amp;ldquo;ABB&amp;rdquo;: 3, &amp;ldquo;BBA&amp;rdquo;: 3, &amp;ldquo;BBBB&amp;rdquo;: 4}
文字列中に含まれる単語の合計得点を文字列の得点とするということにします。
例えば&amp;rdquo;AAAAAAAA&amp;rdquo;なら8点(1 * 8)、&amp;rdquo;AAAAAAAB&amp;rdquo;なら9点(1 * 7 + 2)です。
環境のとりうる状態は長さ8のそれぞれに&amp;rsquo;A&amp;rsquo;, &amp;lsquo;B&amp;rsquo;の2通りあるので2^8通りあり、
アクションは、何もしないのと、位置1~8のそれぞれを&amp;rsquo;A&amp;rsquo;または&amp;rsquo;B&amp;rsquo;なのでを計17通りあります。
今回のコードでは状態は文字列、アクションは整数(0~16)で管理します。
 先述した強化学習の記事では、Q学習の学習中に、
一定の回数遷移を繰り返した後、状態をs0に戻すものとそうでないものがあり、
どちらを採用するか悩んだので QLearning(n_rounds, t_max)として
2重のループにすることで一応どちらの方法でも実行できるようにしました。
これを実行結果はこんな感じ
軸のラベルを書き忘れていますが、
横軸が外側のループが回った数で、縦軸がそれまでに学習したQを使ってt_max回遷移した時のスコアですね。
&amp;ldquo;ABBBBBBB&amp;rdquo;に遷移して終わるのとき最大値28をとるのですが、
約900セット(t_max * 900回)の学習でそれを実現する遷移ができるようになっています。</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>可視化 on ZABURO app</title>
    <link>https://zaburo-ch.github.io/tags/%E5%8F%AF%E8%A6%96%E5%8C%96/</link>
    <description>Recent content in 可視化 on ZABURO app</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <copyright>ZABURO</copyright>
    <lastBuildDate>Wed, 29 Jun 2016 13:06:24 +0900</lastBuildDate>
    
	<atom:link href="https://zaburo-ch.github.io/tags/%E5%8F%AF%E8%A6%96%E5%8C%96/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>t-SNE の逆変換を試してみた</title>
      <link>https://zaburo-ch.github.io/post/tsne-decode/</link>
      <pubDate>Wed, 29 Jun 2016 13:06:24 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/tsne-decode/</guid>
      <description>Parametric t-SNEでt-SNEの変換をニューラルネットで近似することができたので、
その逆についてもやってみました。
逆変換と言っても特に難しいことはやっておらず、
まず普通にBarnes-Hut t-SNEで訓練データを2次元に変換して、
変換後の座標を入力、変換前の座標を教師データとして教師あり学習を行います。
今回は、変換後の座標のうち訓練データにないような座標について、
どのように逆変換されるのかが気になるので汎化性能を高めるためにDropoutを入れました。
コードは次の通りです。

t-SNEの結果がこんな感じで、
適当に座標を指定して逆変換した結果がこれ
かなり綺麗に逆変換できました。
deconvolutionを使えばもっとうまく逆変換できるかもしれませんが、
MNISTの結果としてはこれで十分だと思います。
なんとかしてこの方針で画像生成とかできないのかなーと思っているのですが、
まずはグレースケールでない画像をうまく変換できるようにt-SNE側を工夫しなければいけない気がします。</description>
    </item>
    
    <item>
      <title>Parametric t-SNE を Keras で書いた</title>
      <link>https://zaburo-ch.github.io/post/parametric-tsne-keras/</link>
      <pubDate>Tue, 28 Jun 2016 22:01:32 +0900</pubDate>
      
      <guid>https://zaburo-ch.github.io/post/parametric-tsne-keras/</guid>
      <description>最近、t-SNEについていろいろ調べいて、その中でParametric t-SNEの論文を読みました。
元のt-SNEは可視化や次元削減の手法としてとても有用なのですが、 変換後の座標を乱数で初期化し、
KLダイバージェンスが小さくなるように勾配降下で座標を調整していく感じなので、
初めの乱数次第で配置は大きく変わりますし、別なデータを同じような場所に投射するようなことができません。
そのため、kaggleなどで前処理として使われる際には、
訓練データとテストデータをくっつけて変換するという方法が取られています。
しかし、本来見れないはずのテストデータを訓練データを変換する時にも使うというのはグレーな感じがします。
そこで、座標を直接調整するのではなく、
元の座標をパラメトリックな関数で低次元の座標に投射するようにして、
その関数のパラメータを学習してあげようというのがParametric t-SNEです。
ここで、関数としてニューラルネットが使われます。
論文では、RBMを重ねてpre trainingしてfine tuningというのをやっているのですが、
どうせやるならということで今風にReLUで書きました。
コードはここに置いてあります。
とりあえず論文にも載っているMNISTで試しました。
100 epoch回すとAWS EC2のg2.2xlargeインスタンスでだいたい10分程度かかります。
普通のMNISTなので60000件の訓練データと10000件のテストデータがあります。
学習していく過程が見れたら面白そうだなと思ったので、
各epoch終了後テストデータに対して変換を行い、散布図を書くようにしました。
結果はこんな感じ。
(なんかgifが吐けなくてmp4をgifに変換したので画質が悪い&amp;hellip;)
訓練に使っていないデータに対してすごくいい感じに別けられていると思います。
10~20 epochくらいでいい感じに別けられているので、
10 epoch毎とかにミニバッチをシャッフルしてあげるともっと良くなるかもしれません。
一応shuffle_intervalという変数が用意してあって、
shuffle_interval回のepochが回るとミニバッチがシャッフルされてPが再計算されます。
Pを計算する部分についてもPython上でですが並列化してあるので少しは早いと思います。
Convolutional Parametric t-SNEだー！！って言って畳み込み層を使ったものも書いたのですが、
普通のMLP版とあまり変わらなかったのでお蔵入りしました。
いつもMNISTしてばかりなのでCIFAR-10でも試してみたのですがあまりうまくいきませんでした。
そもそもt-SNEでCIFAR-10がうまくいくのが試していないので良くわかりませんが、
これConvolutionalしてなんとか解決できないかなーと考えています。
追記：
逆変換も試しました。</description>
    </item>
    
  </channel>
</rss>